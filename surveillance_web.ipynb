{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install streamlit pyngrok mediapipe transformers torch torchvision numpy opencv-python-headless ffmpeg-python\n",
        "!apt-get install ffmpeg -y\n",
        "\n",
        "# Install ngrok\n",
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvzf ngrok-v3-stable-linux-amd64.tgz\n",
        "!mv ngrok /usr/local/bin/\n",
        "\n",
        "# Install required packages\n",
        "!pip install mediapipe transformers torch torchvision numpy opencv-python-headless ffmpeg-python\n",
        "!apt-get install ffmpeg -y\n",
        "\n",
        "# Note: You need an ngrok auth token. Sign up at https://ngrok.com, get your token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# Replace 'YOUR_NGROK_AUTH_TOKEN' with your actual token\n",
        "!ngrok authtoken 305NarI0dy6eL1ti16UaQvaQEzE_7qSCpuDQ6DsH7JCxBTzXg"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7V1RTftfS5Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Specify Google Drive output path\n",
        "DRIVE_OUTPUT_PATH = '/content/drive/MyDrive/Surveillance_Output'  # CHANGE THIS TO YOUR DESIRED PATH\n",
        "os.makedirs(DRIVE_OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Define Streamlit app script\n",
        "streamlit_script = \"\"\"\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Streamlit app configuration\n",
        "st.set_page_config(page_title=\"Enhanced Retail Surveillance\", layout=\"wide\")\n",
        "st.title(\"Enhanced Retail Surveillance System\")\n",
        "st.write(\"Upload a video to analyze behavior using VideoMAE.\")\n",
        "\n",
        "# Video upload\n",
        "uploaded_file = st.file_uploader(\"Choose a video file\", type=[\"mp4\", \"webm\", \"avi\", \"mov\"])\n",
        "if uploaded_file is not None:\n",
        "    # Save uploaded video to /content/\n",
        "    input_video_path = os.path.join(\"/content\", uploaded_file.name)\n",
        "    try:\n",
        "        with open(input_video_path, \"wb\") as f:\n",
        "            f.write(uploaded_file.getvalue())\n",
        "        st.success(f\"✅ Video uploaded: {uploaded_file.name}\")\n",
        "        # Write processing flag\n",
        "        process_flag = \"/content/process_video.txt\"\n",
        "        with open(process_flag, \"w\") as f:\n",
        "            f.write(input_video_path)\n",
        "        st.info(\"Processing video... Re-run Cell 3 in Colab, then refresh this page to see results.\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error saving video: {str(e)}\")\n",
        "\n",
        "# Display results\n",
        "result_path = \"/content/drive/MyDrive/Surveillance_Output/result.json\"\n",
        "process_flag = \"/content/process_video.txt\"\n",
        "video_file_name = \"unknown_video\"\n",
        "if os.path.exists(process_flag):\n",
        "    try:\n",
        "        with open(process_flag, \"r\") as f:\n",
        "            input_video_path = f.read().strip()\n",
        "            video_file_name = os.path.basename(input_video_path)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Error reading process flag: {str(e)}\")\n",
        "if os.path.exists(result_path):\n",
        "    try:\n",
        "        with open(result_path, \"r\") as f:\n",
        "            result_data = json.load(f)\n",
        "        violence_detected = result_data.get(\"violence_detected\", 0)\n",
        "        result_text = \"Violence Detected\" if violence_detected >= 1 else \"Normal\"\n",
        "        formatted_result = f\"{result_text}\"\n",
        "        st.subheader(formatted_result)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading result: {str(e)}\")\n",
        "        logging.error(f\"❌ Error loading result.json: {str(e)}\")\n",
        "else:\n",
        "    st.info(\"No video processed yet. Upload a video to start.\")\n",
        "\"\"\"\n",
        "\n",
        "# Save Streamlit script\n",
        "try:\n",
        "    with open('/content/surveillance_streamlit_app.py', 'w') as f:\n",
        "        f.write(streamlit_script)\n",
        "    print(f\"✅ Saved Streamlit app to /content/surveillance_streamlit_app.py\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error saving Streamlit script: {str(e)}\")"
      ],
      "metadata": {
        "id": "_DpQv-DZYmmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Imports\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
        "import torchvision.transforms as T\n",
        "from collections import deque\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Specify Google Drive output path\n",
        "DRIVE_OUTPUT_PATH = '/content/drive/MyDrive/Surveillance_Output'  # CHANGE THIS TO MATCH YOUR PATH\n",
        "os.makedirs(DRIVE_OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Surveillance processing classes\n",
        "class PersonTracker:\n",
        "    \"\"\"Enhanced person tracker with improved detection and tracking\"\"\"\n",
        "    def __init__(self, max_disappeared=30, max_distance=100):\n",
        "        self.next_id = 0\n",
        "        self.objects = {}\n",
        "        self.disappeared = {}\n",
        "        self.max_disappeared = max_disappeared\n",
        "        self.max_distance = max_distance\n",
        "\n",
        "    def register(self, centroid, bbox):\n",
        "        self.objects[self.next_id] = {\n",
        "            'centroid': centroid,\n",
        "            'bbox': bbox,\n",
        "            'positions': deque([centroid], maxlen=50),\n",
        "            'behavior_history': deque(maxlen=10),\n",
        "            'last_behavior': 'Normal',\n",
        "            'behavior_score': 0.0,\n",
        "            'frame_count': 0\n",
        "        }\n",
        "        self.disappeared[self.next_id] = 0\n",
        "        self.next_id += 1\n",
        "\n",
        "    def deregister(self, object_id):\n",
        "        del self.objects[object_id]\n",
        "        del self.disappeared[object_id]\n",
        "\n",
        "    def update(self, detections):\n",
        "        if len(detections) == 0:\n",
        "            for object_id in list(self.disappeared.keys()):\n",
        "                self.disappeared[object_id] += 1\n",
        "                if self.disappeared[object_id] > self.max_disappeared:\n",
        "                    self.deregister(object_id)\n",
        "            return self.objects\n",
        "\n",
        "        input_centroids = np.array([det['centroid'] for det in detections])\n",
        "        input_bboxes = [det['bbox'] for det in detections]\n",
        "\n",
        "        if len(self.objects) == 0:\n",
        "            for i, detection in enumerate(detections):\n",
        "                self.register(input_centroids[i], input_bboxes[i])\n",
        "        else:\n",
        "            object_centroids = np.array([obj['centroid'] for obj in self.objects.values()])\n",
        "            object_ids = list(self.objects.keys())\n",
        "            D = np.linalg.norm(object_centroids[:, np.newaxis] - input_centroids, axis=2)\n",
        "\n",
        "            if D.shape[0] <= D.shape[1]:\n",
        "                rows, cols = linear_sum_assignment(D)\n",
        "            else:\n",
        "                rows, cols = linear_sum_assignment(D.T)\n",
        "                rows, cols = cols, rows\n",
        "\n",
        "            used_row_indices = set()\n",
        "            used_col_indices = set()\n",
        "\n",
        "            for (row, col) in zip(rows, cols):\n",
        "                if D[row, col] <= self.max_distance:\n",
        "                    object_id = object_ids[row]\n",
        "                    self.objects[object_id]['centroid'] = input_centroids[col]\n",
        "                    self.objects[object_id]['bbox'] = input_bboxes[col]\n",
        "                    self.objects[object_id]['positions'].append(input_centroids[col])\n",
        "                    self.objects[object_id]['frame_count'] += 1\n",
        "                    self.disappeared[object_id] = 0\n",
        "                    used_row_indices.add(row)\n",
        "                    used_col_indices.add(col)\n",
        "\n",
        "            unused_row_indices = set(range(0, D.shape[0])).difference(used_row_indices)\n",
        "            unused_col_indices = set(range(0, D.shape[1])).difference(used_col_indices)\n",
        "\n",
        "            for row in unused_row_indices:\n",
        "                object_id = object_ids[row]\n",
        "                self.disappeared[object_id] += 1\n",
        "                if self.disappeared[object_id] > self.max_disappeared:\n",
        "                    self.deregister(object_id)\n",
        "\n",
        "            for col in unused_col_indices:\n",
        "                self.register(input_centroids[col], input_bboxes[col])\n",
        "\n",
        "        return self.objects\n",
        "\n",
        "class EnhancedRetailSurveillanceSystem:\n",
        "    def __init__(self, drive_output_path=DRIVE_OUTPUT_PATH):\n",
        "        print(\"🏪 Initializing Enhanced Retail Surveillance System with VideoMAE...\")\n",
        "        self.drive_output_path = drive_output_path\n",
        "        try:\n",
        "            os.makedirs(self.drive_output_path, exist_ok=True)\n",
        "            print(f\"✅ Created/Verified Google Drive output directory: {self.drive_output_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error creating Google Drive directory {self.drive_output_path}: {e}\")\n",
        "            raise\n",
        "        self.setup_models()\n",
        "        self.setup_detection_parameters()\n",
        "        self.frame_buffer = {}\n",
        "        self.buffer_size = 16  # Match reference code\n",
        "        self.person_tracker = PersonTracker(max_disappeared=30, max_distance=100)\n",
        "        self.setup_person_detection()\n",
        "        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(\n",
        "            history=500, varThreshold=16, detectShadows=True\n",
        "        )\n",
        "        self.activity_log = []\n",
        "        self.violence_alerts = []\n",
        "        self.detection_stats = {\n",
        "            'total_frames': 0,\n",
        "            'violence_detected': 0,\n",
        "            'abnormal_behavior': 0,\n",
        "            'normal_behavior': 0\n",
        "        }\n",
        "        print(\"✅ Enhanced Retail Surveillance System initialized!\")\n",
        "\n",
        "    def setup_models(self):\n",
        "        print(\"🤖 Setting up AI models...\")\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if self.device.type == \"cpu\":\n",
        "            print(\"⚠️ No GPU available, processing will be slower\")\n",
        "        self.violence_model = VideoMAEForVideoClassification.from_pretrained(\n",
        "            \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
        "        ).to(self.device)\n",
        "        self.violence_model.eval()\n",
        "        self.image_processor = VideoMAEImageProcessor.from_pretrained(\n",
        "            \"MCG-NJU/videomae-base-finetuned-kinetics\", use_fast=True\n",
        "        )\n",
        "        self.behavior_classes = ['Normal', 'Suspicious', 'Aggressive', 'Violence', 'Theft']\n",
        "        self.class_mapping = self.create_action_mapping()\n",
        "        self.behavior_thresholds = {\n",
        "            'Normal': 0.4,\n",
        "            'Suspicious': 0.4,\n",
        "            'Aggressive': 0.4,\n",
        "            'Violence': 0.6,\n",
        "            'Theft': 0.4\n",
        "        }\n",
        "\n",
        "    def create_action_mapping(self):\n",
        "        \"\"\"Map Kinetics-400 classes to 5 target classes\"\"\"\n",
        "        kinetics_labels = self.violence_model.config.id2label\n",
        "        mapping = {}\n",
        "        for idx, label in kinetics_labels.items():\n",
        "            label_lower = label.lower()\n",
        "            if any(keyword in label_lower for keyword in ['fight', 'punch', 'kick', 'attack', 'hit']):\n",
        "                mapping[idx] = 'Violence'\n",
        "            elif any(keyword in label_lower for keyword in ['run away', 'sneak', 'hide']):\n",
        "                mapping[idx] = 'Suspicious'\n",
        "            elif any(keyword in label_lower for keyword in ['push', 'shout', 'argue', 'confront']):\n",
        "                mapping[idx] = 'Aggressive'\n",
        "            elif any(keyword in label_lower for keyword in ['steal', 'shoplift', 'pickpocket']):\n",
        "                mapping[idx] = 'Theft'\n",
        "            else:\n",
        "                mapping[idx] = 'Normal'\n",
        "        return mapping\n",
        "\n",
        "    def setup_person_detection(self):\n",
        "        \"\"\"Setup person detection using HOG\"\"\"\n",
        "        self.hog = cv2.HOGDescriptor()\n",
        "        self.hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "        try:\n",
        "            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "            self.has_face_detection = True\n",
        "        except:\n",
        "            self.has_face_detection = False\n",
        "            print(\"⚠️ Face detection not available\")\n",
        "\n",
        "    def setup_detection_parameters(self):\n",
        "        \"\"\"Setup detection parameters\"\"\"\n",
        "        self.min_person_area = 2000\n",
        "        self.max_person_area = 50000\n",
        "        self.min_aspect_ratio = 0.3\n",
        "        self.max_aspect_ratio = 3.0\n",
        "        self.behavior_window = 10\n",
        "        self.violence_threshold = 0.6\n",
        "        self.suspicious_threshold = 0.4\n",
        "\n",
        "    def detect_people(self, frame):\n",
        "        \"\"\"Detect people in the frame using HOG\"\"\"\n",
        "        detections = []\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        try:\n",
        "            (rects, weights) = self.hog.detectMultiScale(\n",
        "                gray, winStride=(4, 4), padding=(8, 8), scale=1.05, hitThreshold=0.5\n",
        "            )\n",
        "            for i, (x, y, w, h) in enumerate(rects):\n",
        "                area = w * h\n",
        "                aspect_ratio = h / w if w > 0 else 0\n",
        "                if (self.min_person_area <= area <= self.max_person_area and\n",
        "                    self.min_aspect_ratio <= aspect_ratio <= self.max_aspect_ratio):\n",
        "                    confidence = weights[i] if i < len(weights) else 0.5\n",
        "                    detections.append({\n",
        "                        'bbox': [x, y, w, h],\n",
        "                        'centroid': (x + w//2, y + h//2),\n",
        "                        'confidence': confidence,\n",
        "                        'method': 'HOG'\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"HOG detection error: {e}\")\n",
        "        if len(detections) == 0:\n",
        "            detections.extend(self.detect_motion_based_people(frame))\n",
        "        detections = self.remove_overlapping_detections(detections)\n",
        "        return detections\n",
        "\n",
        "    def detect_motion_based_people(self, frame):\n",
        "        \"\"\"Detect people using motion analysis\"\"\"\n",
        "        detections = []\n",
        "        fg_mask = self.bg_subtractor.apply(frame)\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)\n",
        "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
        "        contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        for contour in contours:\n",
        "            area = cv2.contourArea(contour)\n",
        "            if area > self.min_person_area:\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "                aspect_ratio = h / w if w > 0 else 0\n",
        "                if (self.min_aspect_ratio <= aspect_ratio <= self.max_aspect_ratio and\n",
        "                    area <= self.max_person_area):\n",
        "                    detections.append({\n",
        "                        'bbox': [x, y, w, h],\n",
        "                        'centroid': (x + w//2, y + h//2),\n",
        "                        'confidence': 0.4,\n",
        "                        'method': 'Motion'\n",
        "                    })\n",
        "        return detections\n",
        "\n",
        "    def remove_overlapping_detections(self, detections, overlap_threshold=0.5):\n",
        "        \"\"\"Remove overlapping detections using NMS\"\"\"\n",
        "        if len(detections) <= 1:\n",
        "            return detections\n",
        "        boxes = [[x, y, x+w, y+h] for x, y, w, h in [det['bbox'] for det in detections]]\n",
        "        confidences = [det['confidence'] for det in detections]\n",
        "        boxes = np.array(boxes)\n",
        "        confidences = np.array(confidences)\n",
        "        indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), 0.3, overlap_threshold)\n",
        "        if len(indices) > 0:\n",
        "            indices = indices.flatten()\n",
        "            return [detections[i] for i in indices]\n",
        "        return []\n",
        "\n",
        "    def analyze_behavior(self, frame, person_bbox, person_id):\n",
        "        \"\"\"Analyze behavior for a person region using a sequence of frames\"\"\"\n",
        "        try:\n",
        "            x, y, w, h = person_bbox\n",
        "            padding = 20\n",
        "            x1 = max(0, x - padding)\n",
        "            y1 = max(0, y - padding)\n",
        "            x2 = min(frame.shape[1], x + w + padding)\n",
        "            y2 = min(frame.shape[0], y + h + padding)\n",
        "            person_region = frame[x1:x2, y1:y2]  # Corrected order to match frame indexing\n",
        "            if person_region.size == 0 or person_region.shape[0] < 10 or person_region.shape[1] < 10:\n",
        "                logging.warning(f\"Person {person_id}: Invalid region (shape: {person_region.shape})\")\n",
        "                return {'behavior': 'Normal', 'confidence': 0.5, 'score': 0.0}\n",
        "\n",
        "            person_region = cv2.cvtColor(person_region, cv2.COLOR_BGR2RGB)\n",
        "            pil_image = Image.fromarray(person_region)\n",
        "\n",
        "            if person_id not in self.frame_buffer:\n",
        "                self.frame_buffer[person_id] = deque(maxlen=self.buffer_size)\n",
        "            self.frame_buffer[person_id].append(pil_image)\n",
        "\n",
        "            if len(self.frame_buffer[person_id]) < self.buffer_size:\n",
        "                logging.info(f\"Person {person_id}: Waiting for {self.buffer_size} frames, currently {len(self.frame_buffer[person_id])}\")\n",
        "                return {'behavior': 'Normal', 'confidence': 0.5, 'score': 0.0}\n",
        "\n",
        "            # Ensure all frames are resized to 224x224\n",
        "            resized_frames = []\n",
        "            for img in self.frame_buffer[person_id]:\n",
        "                img_resized = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
        "                resized_frames.append(img_resized)\n",
        "\n",
        "            inputs = self.image_processor(\n",
        "                resized_frames, return_tensors=\"pt\", num_frames=self.buffer_size\n",
        "            )\n",
        "            input_tensor = inputs['pixel_values'].to(self.device)\n",
        "            logging.info(f\"Person {person_id}: Input tensor shape: {input_tensor.shape}\")\n",
        "            with torch.no_grad():\n",
        "                outputs = self.violence_model(input_tensor)\n",
        "                logits = outputs.logits\n",
        "                behavior_prob = torch.nn.functional.softmax(logits, dim=1)\n",
        "                behavior_pred = torch.argmax(behavior_prob, dim=1).item()\n",
        "                behavior_score = behavior_prob[0, behavior_pred].item()\n",
        "                predicted_behavior = self.class_mapping.get(behavior_pred, 'Normal')\n",
        "                logging.info(f\"Person {person_id}: Predicted {predicted_behavior} (score: {behavior_score:.2f})\")\n",
        "                if behavior_score < 0.3:\n",
        "                    predicted_behavior = 'Normal'\n",
        "                    behavior_score = 0.3\n",
        "                return {\n",
        "                    'behavior': predicted_behavior,\n",
        "                    'confidence': behavior_score,\n",
        "                    'score': behavior_score\n",
        "                }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Person {person_id}: Behavior analysis error: {e}\")\n",
        "            return {'behavior': 'Normal', 'confidence': 0.5, 'score': 0.0}\n",
        "\n",
        "    def update_behavior_history(self, person_id, behavior_result):\n",
        "        \"\"\"Update behavior history for a person\"\"\"\n",
        "        if person_id in self.person_tracker.objects:\n",
        "            person = self.person_tracker.objects[person_id]\n",
        "            person['behavior_history'].append(behavior_result)\n",
        "            if len(person['behavior_history']) >= 3:\n",
        "                recent_behaviors = list(person['behavior_history'])[-5:]\n",
        "                behavior_counts = {}\n",
        "                for behavior in recent_behaviors:\n",
        "                    behavior_type = behavior['behavior']\n",
        "                    behavior_counts[behavior_type] = behavior_counts.get(behavior_type, 0) + 1\n",
        "                dominant_behavior = max(behavior_counts, key=behavior_counts.get)\n",
        "                person['last_behavior'] = dominant_behavior\n",
        "                person['behavior_score'] = np.mean([b['score'] for b in recent_behaviors])\n",
        "\n",
        "    def draw_person_annotations(self, frame, tracked_people):\n",
        "        \"\"\"Draw bounding boxes and behavior labels for tracked people\"\"\"\n",
        "        annotated_frame = frame.copy()\n",
        "        for person_id, person_data in tracked_people.items():\n",
        "            x, y, w, h = person_data['bbox']\n",
        "            behavior = person_data['last_behavior']\n",
        "            behavior_score = person_data['behavior_score']\n",
        "            color = {\n",
        "                'Violence': (0, 0, 255),\n",
        "                'Aggressive': (0, 100, 255),\n",
        "                'Suspicious': (0, 255, 255),\n",
        "                'Theft': (255, 0, 255),\n",
        "                'Normal': (0, 255, 0)\n",
        "            }.get(behavior, (0, 255, 0))\n",
        "            thickness = 3 if behavior in ['Violence', 'Aggressive', 'Theft'] else 2\n",
        "            cv2.rectangle(annotated_frame, (x, y), (x+w, y+h), color, thickness)\n",
        "            label = f\"{behavior} ({behavior_score:.2f})\" if behavior_score > 0 else behavior\n",
        "            font_scale = 0.7\n",
        "            font_thickness = 2\n",
        "            (text_width, text_height), baseline = cv2.getTextSize(\n",
        "                label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness\n",
        "            )\n",
        "            cv2.rectangle(annotated_frame,\n",
        "                         (x, y - text_height - 10),\n",
        "                         (x + text_width, y),\n",
        "                         color, -1)\n",
        "            cv2.putText(annotated_frame, label, (x, y - 5),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)\n",
        "            cv2.putText(annotated_frame, f\"ID:{person_id}\", (x, y + h + 20),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "        return annotated_frame\n",
        "\n",
        "    def process_frame(self, frame, frame_count):\n",
        "        \"\"\"Process a single frame for person detection and behavior analysis\"\"\"\n",
        "        detections = self.detect_people(frame)\n",
        "        tracked_people = self.person_tracker.update(detections)\n",
        "        alerts = []\n",
        "\n",
        "        for person_id, person_data in tracked_people.items():\n",
        "            behavior_result = self.analyze_behavior(frame, person_data['bbox'], person_id)\n",
        "            self.update_behavior_history(person_id, behavior_result)\n",
        "            if behavior_result['behavior'] in ['Violence', 'Aggressive', 'Theft']:\n",
        "                alerts.append({\n",
        "                    'type': behavior_result['behavior'],\n",
        "                    'person_id': person_id,\n",
        "                    'frame': frame_count,\n",
        "                    'confidence': behavior_result['confidence']\n",
        "                })\n",
        "                if behavior_result['behavior'] == 'Violence':\n",
        "                    self.detection_stats['violence_detected'] += 1\n",
        "                self.detection_stats['abnormal_behavior'] += 1\n",
        "            else:\n",
        "                self.detection_stats['normal_behavior'] += 1\n",
        "\n",
        "        annotated_frame = self.draw_person_annotations(frame, tracked_people)\n",
        "        return annotated_frame, {'alerts': alerts}\n",
        "\n",
        "    def process_video(self, video_source=0, output_video=None, max_frames=100, display_frames=True, colab_mode=True):\n",
        "        \"\"\"Main video processing loop\"\"\"\n",
        "        print(f\"🎥 Starting video processing from source: {video_source}\")\n",
        "        if isinstance(video_source, str) and video_source.endswith('.webm'):\n",
        "            temp_path = os.path.join(self.drive_output_path, 'temp.mp4')\n",
        "            os.system(f'ffmpeg -i {video_source} -c:v libx264 -c:a aac {temp_path}')\n",
        "            if os.path.exists(temp_path):\n",
        "                video_source = temp_path\n",
        "                print(f\"✅ Converted .webm to .mp4: {video_source}\")\n",
        "            else:\n",
        "                print(\"❌ Failed to convert .webm to .mp4\")\n",
        "                return False\n",
        "        cap = cv2.VideoCapture(video_source)\n",
        "        if not cap.isOpened():\n",
        "            print(\"❌ Error: Could not open video source\")\n",
        "            return False\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        print(f\"📊 Video properties: {width}x{height} @ {fps}fps\")\n",
        "        out = None\n",
        "        if output_video:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
        "        frame_count = 0\n",
        "        start_time = time.time()\n",
        "        print(\"\\n🚀 Starting surveillance processing...\")\n",
        "        try:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    print(\"📺 End of video or camera disconnected\")\n",
        "                    break\n",
        "                if max_frames and frame_count >= max_frames:\n",
        "                    print(f\"🎯 Reached maximum frames limit: {max_frames}\")\n",
        "                    break\n",
        "                frame_count += 1\n",
        "                self.detection_stats['total_frames'] = frame_count\n",
        "                annotated_frame, results = self.process_frame(frame, frame_count)\n",
        "                if display_frames and colab_mode:\n",
        "                    try:\n",
        "                        from google.colab.patches import cv2_imshow\n",
        "                        if frame_count % 30 == 0:\n",
        "                            print(f\"📺 Frame {frame_count}:\")\n",
        "                            cv2_imshow(annotated_frame)\n",
        "                    except ImportError:\n",
        "                        print(\"⚠️ Could not import cv2_imshow, disabling frame display\")\n",
        "                if out:\n",
        "                    out.write(annotated_frame)\n",
        "                if results['alerts']:\n",
        "                    for alert in results['alerts']:\n",
        "                        print(f\"🚨 ALERT: {alert['type']} detected for Person {alert['person_id']} \"\n",
        "                              f\"(Frame {alert['frame']}, Confidence: {alert['confidence']:.2f})\")\n",
        "                if frame_count % 100 == 0:\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    fps_current = frame_count / elapsed_time\n",
        "                    print(f\"📊 Processing FPS: {fps_current:.1f}, Frame: {frame_count}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during processing: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "        finally:\n",
        "            cap.release()\n",
        "            if out:\n",
        "                out.release()\n",
        "            total_time = time.time() - start_time\n",
        "            print(f\"\\n📈 FINAL STATISTICS:\")\n",
        "            print(f\"   • Total Processing Time: {total_time:.1f} seconds\")\n",
        "            print(f\"   • Frames Processed: {frame_count}\")\n",
        "            print(f\"   • Violence Detected: {self.detection_stats['violence_detected']}\")\n",
        "            print(f\"   • Normal Behavior: {self.detection_stats['normal_behavior']}\")\n",
        "            print(f\"   • Abnormal Behavior: {self.detection_stats['abnormal_behavior']}\")\n",
        "            print(\"✅ Surveillance system shutdown complete\")\n",
        "            stats_path = os.path.join(self.drive_output_path, 'stats.json')\n",
        "            try:\n",
        "                with open(stats_path, 'w') as f:\n",
        "                    json.dump(self.detection_stats, f)\n",
        "                print(f\"✅ Statistics saved at {stats_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error saving stats.json to {stats_path}: {e}\")\n",
        "            return self.detection_stats['violence_detected']\n",
        "\n",
        "# Process video if flag exists\n",
        "process_flag = '/content/process_video.txt'\n",
        "if os.path.exists(process_flag):\n",
        "    with open(process_flag, 'r') as f:\n",
        "        input_video_path = f.read().strip()\n",
        "    if os.path.exists(input_video_path):\n",
        "        output_video = os.path.join(DRIVE_OUTPUT_PATH, f\"surveillance_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4\")\n",
        "        print(f\"Processing video: {input_video_path}\")\n",
        "        # Copy input video to Google Drive\n",
        "        input_video_drive_path = os.path.join(DRIVE_OUTPUT_PATH, os.path.basename(input_video_path))\n",
        "        try:\n",
        "            shutil.copy(input_video_path, input_video_drive_path)\n",
        "            print(f\"✅ Input video copied to {input_video_drive_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error copying input video to {input_video_drive_path}: {e}\")\n",
        "        system = EnhancedRetailSurveillanceSystem(drive_output_path=DRIVE_OUTPUT_PATH)\n",
        "        violence_detected = system.process_video(\n",
        "            video_source=input_video_path,\n",
        "            output_video=output_video,\n",
        "            max_frames=100,\n",
        "            colab_mode=True\n",
        "        )\n",
        "        if violence_detected is not False:\n",
        "            result_text = 'Violence Detected' if violence_detected >= 1 else 'Normal - Violence Not Detected'\n",
        "            print(f\"📢 Result: {result_text}\")\n",
        "            result_path = os.path.join(DRIVE_OUTPUT_PATH, 'result.json')\n",
        "            result_data = {'violence_detected': violence_detected}\n",
        "            try:\n",
        "                with open(result_path, 'w') as f:\n",
        "                    json.dump(result_data, f)\n",
        "                print(f\"✅ Result saved at {result_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error saving result.json: {e}\")\n",
        "        else:\n",
        "            print(\"❌ Video processing failed. Check logs for details.\")\n",
        "        # Remove process flag\n",
        "        os.remove(process_flag)\n",
        "    else:\n",
        "        print(f\"❌ Video file not found: {input_video_path}\")\n",
        "else:\n",
        "    print(\"No video to process. Upload a video in the Streamlit app and re-run this cell.\")"
      ],
      "metadata": {
        "id": "QJSI6o3B52cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Terminate existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"🌐 Streamlit app running at: {public_url}\")\n",
        "\n",
        "# Run Streamlit app with file watcher disabled\n",
        "!streamlit run /content/surveillance_streamlit_app.py --server.port 8501 --server.address 0.0.0.0 --server.fileWatcherType none"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j_BURhB7ex1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}